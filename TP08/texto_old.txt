## üìö Multinomial Naive Bayes ‚Äî Marco Te√≥rico

### üî∏ Introducci√≥n

El clasificador **Multinomial Naive Bayes (MNB)** es un modelo generativo que asume que los datos (por ejemplo, documentos de texto) est√°n compuestos por **conteos discretos** de eventos (como palabras o tokens) y que estos conteos se generan a partir de una **distribuci√≥n multinomial** condicionada a la clase del documento.

---

### üî∏ Supuestos del modelo

Sea:

- \( \mathcal{C} = \{c_1, \dots, c_K\} \): conjunto de clases posibles.
- \( \boldsymbol{\theta}^{(k)} = (\theta_1^{(k)}, \dots, \theta_V^{(k)}) \): par√°metros de la distribuci√≥n multinomial para la clase \( c_k \), donde \( V \) es el tama√±o del vocabulario.
- \( \mathbf{N} = (N_1, \dots, N_V) \): vector de conteos de cada token en un documento.
- \( d = \sum_{m=1}^V N_m \): total de tokens en el documento (longitud del documento).

El modelo asume que dado que un documento pertenece a una clase \( Y = k \), sus conteos se generan como:

\[
\mathbf{N} \mid Y = k \sim \text{Multinomial}(d, \boldsymbol{\theta}^{(k)})
\]

---

### üî∏ Inferencia

Para clasificar un nuevo documento con vector de conteos \( \mathbf{x} \), se aplica la regla de Bayes:

\[
p(y \mid \mathbf{x}) \propto p(y) \cdot \prod_{j=1}^V \theta_j^{(y)}{}^{x_j}
\]

o, de forma logar√≠tmica:

\[
\log p(y \mid \mathbf{x}) \propto \log p(y) + \sum_{j=1}^V x_j \cdot \log \theta_j^{(y)}
\]

---

### üî∏ Estimaci√≥n de par√°metros (Frecuentista)

Los par√°metros de clase se estiman por m√°xima verosimilitud:

\[
\hat{c}_k = \frac{\#\{y_i = k\}}{n}
\]

Y para los par√°metros de la distribuci√≥n multinomial:

\[
\tilde{N}_m^{(k)} = \sum_{i=1}^n N_{i,m} \cdot \mathbb{1}_{\{y_i = k\}}
\]

\[
\hat{\theta}_m^{(k)} = \frac{\tilde{N}_m^{(k)}}{\sum_{m'} \tilde{N}_{m'}^{(k)}}
\]

Este estimador puede dar lugar a ceros si un t√©rmino no aparece en los datos de clase \( k \), lo que motiva el uso de suavizado.

---

### üî∏ Consideraci√≥n sobre el valor \( d \)

Cada muestra (documento) puede tener una longitud distinta \( d \). Esto es t√≠pico en texto, donde cada documento posee una cantidad diferente de palabras.

---

### üî∏ Enfoque Bayesiano

Para evitar problemas de probabilidad cero y mejorar la generalizaci√≥n, se puede usar un enfoque bayesiano. Se asume:

\[
\boldsymbol{\theta}^{(k)} \sim \text{Dir}(\alpha_1, \dots, \alpha_V)
\]

y el modelo jer√°rquico completo es:

\[
(\tilde{N}_1^{(k)}, \dots, \tilde{N}_V^{(k)}) \mid \boldsymbol{\theta}^{(k)} \sim \text{Multinomial}(d^{(k)}, \boldsymbol{\theta}^{(k)})
\]

---

### üî∏ Distribuci√≥n Dirichlet

La Dirichlet es una beta multivariada con densidad:

\[
p(\theta_1, \dots, \theta_V) = \frac{\Gamma(\sum_m \alpha_m)}{\prod_m \Gamma(\alpha_m)} \prod_m \theta_m^{\alpha_m - 1}
\quad \text{con} \quad \sum_m \theta_m = 1, \theta_m \geq 0
\]

Sus marginales son:

\[
T_m \sim \text{Beta}(\alpha_m, \sum_{j \neq m} \alpha_j)
\quad \Rightarrow \quad \mathbb{E}[T_m] = \frac{\alpha_m}{\sum_j \alpha_j}
\]

---

### üî∏ Estimaci√≥n Bayesiana con Suavizado

Gracias a la conjugaci√≥n de la Dirichlet con la Multinomial, el posterior es:

\[
\boldsymbol{\theta}^{(k)} \mid \text{datos} \sim \text{Dir}(\alpha_1 + \tilde{N}_1^{(k)}, \dots, \alpha_V + \tilde{N}_V^{(k)})
\]

y el estimador de m√°xima a posteriori es:

\[
\hat{\theta}_m^{(k)} = \frac{\tilde{N}_m^{(k)} + \alpha_m}{\sum_{m'} (\tilde{N}_{m'}^{(k)} + \alpha_{m'})}
\]

Para suavizado de Laplace se suele usar \( \alpha_m = 1 \) para todo \( m \).

---
