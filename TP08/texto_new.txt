#  Clasificaci贸n con **Multinomial Naive Bayes**

El clasificador **Multinomial Naive Bayes (MNB)** es un modelo probabil铆stico que se basa en la f贸rmula de Bayes, bajo el supuesto de independencia condicional entre caracter铆sticas dado el valor de la clase.

Este modelo es especialmente adecuado para problemas con datos representados como **conteos discretos**, como el caso del modelado de texto con bolsas de palabras (Bag-of-Words).

El objetivo es modelar:

$$
p(y \mid \mathbf{x}) \propto p(y) \cdot \prod_{j=1}^{V} \theta^{(y)}_j{}^{x_j}
$$

donde:
- $ y $ es una clase posible.
- $ \mathbf{x} = (x_1, \dots, x_V) $ es el vector de conteos de t茅rminos (como en una bolsa de palabras).
- $ \theta^{(y)}_j $ es la probabilidad de la palabra $ j $ dado que el documento es de clase $ y $.

---

##  Fundamento te贸rico

Seg煤n las filminas, el modelo asume que los datos se generan como:

$$
\mathbf{N} \mid Y = k \sim \text{Multinomial}(d^{(k)}, \theta^{(k)}),
$$

donde:
- $ \mathbf{N} = (N_1, \dots, N_V) $ es el vector de conteos para un documento.
- $ \theta^{(k)} \sim \text{Dir}(\alpha) $ es una distribuci贸n Dirichlet que regulariza las probabilidades de palabras.
- $ d^{(k)} $ es la suma de ocurrencias en todos los documentos de clase $ k $.

El estimador bayesiano para los par谩metros es:

$$
\hat{\theta}^{(k)}_j = \frac{N^{(k)}_j + \alpha}{\sum_m N^{(k)}_m + V \cdot \alpha}
$$

Y la inferencia se realiza mediante:

$$
\log p(y \mid \mathbf{x}) \propto \log p(y) + \sum_j x_j \cdot \log \theta^{(y)}_j
$$

---

## 锔 Clase `MNB`

Se implement贸 una clase `MNB` que encapsula el entrenamiento y predicci贸n con el clasificador Multinomial Naive Bayes, utilizando suavizado de Laplace (Dirichlet uniforme).

### **Inicializaci贸n**

Para inicializar el modelo se utiliza:

**MNB(alpha=1.0)**

donde `alpha` es el par谩metro de suavizado que regulariza las probabilidades $ \theta^{(y)}_j $.

---

### `fit(X, y)`

Entrena el modelo con:
- `X`: matriz de conteos (documentos x palabras).
- `y`: vector de etiquetas (una por documento).

Durante el entrenamiento:
- Se estima la **probabilidad a priori** de cada clase:
  
  $$
  \log p(y = k) = \log \frac{\#\text{docs con clase } k}{\text{total de docs}}
  $$

- Se acumulan los conteos totales por clase y se estima:

  $$
  \log \theta^{(k)}_j = \log \left( \frac{\text{ocurrencias de la palabra } j \text{ en clase } k + \alpha}{\text{total de palabras en clase } k + V \cdot \alpha} \right)
  $$

---

### `predict(X)`

Devuelve las clases predichas para cada documento (fila de `X`), eligiendo:

$$
\hat{y} = \arg\max_y \left[ \log p(y) + \sum_j x_j \log \theta^{(y)}_j \right]
$$

---

### `predict_proba(X)`

Devuelve la **distribuci贸n de probabilidad** sobre las clases para cada documento:

$$
p(y \mid \mathbf{x}) = \frac{e^{\text{score}_y}}{\sum_{y'} e^{\text{score}_{y'}}}
$$

donde `score_y` es el logaritmo de la probabilidad no normalizada.

---

### `score(X, y)`

Devuelve el **accuracy** sobre el conjunto de evaluaci贸n:

$$
\frac{\# \text{predicciones correctas}}{\# \text{total de ejemplos}}
$$

---

### `macro_f1(X, y)`

Devuelve el **F1 macro** promedio entre clases, penalizando la clase desbalanceada. Se usa:

$$
\text{Macro-F1} = \frac{1}{K} \sum_{k=1}^K \text{F1-score}_k
$$

---

##  Aplicaci贸n t铆pica

Este modelo es com煤nmente utilizado en:
- Clasificaci贸n de texto (spam vs. ham, noticias, opiniones).
- An谩lisis de sentimientos.
- Detecci贸n de temas.

Gracias a su simplicidad y efectividad, MNB es un gran modelo base para tareas donde los datos son representaciones de **frecuencias de tokens discretos**.
