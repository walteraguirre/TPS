# 🔍 Clasificación con **Multinomial Naive Bayes**

El clasificador **Multinomial Naive Bayes (MNB)** es un modelo probabilístico que se basa en la fórmula de Bayes, bajo el supuesto de independencia condicional entre características dado el valor de la clase.

Este modelo es especialmente adecuado para problemas con datos representados como **conteos discretos**, como el caso del modelado de texto con bolsas de palabras (Bag-of-Words).

El objetivo es modelar:

$$
p(y \mid \mathbf{x}) \propto p(y) \cdot \prod_{j=1}^{V} \theta^{(y)}_j{}^{x_j}
$$

donde:
- $ y $ es una clase posible.
- $ \mathbf{x} = (x_1, \dots, x_V) $ es el vector de conteos de términos (como en una bolsa de palabras).
- $ \theta^{(y)}_j $ es la probabilidad de la palabra $ j $ dado que el documento es de clase $ y $.

---

## 🎓 Fundamento teórico

Según las filminas, el modelo asume que los datos se generan como:

$$
\mathbf{N} \mid Y = k \sim \text{Multinomial}(d^{(k)}, \theta^{(k)}),
$$

donde:
- $ \mathbf{N} = (N_1, \dots, N_V) $ es el vector de conteos para un documento.
- $ \theta^{(k)} \sim \text{Dir}(\alpha) $ es una distribución Dirichlet que regulariza las probabilidades de palabras.
- $ d^{(k)} $ es la suma de ocurrencias en todos los documentos de clase $ k $.

El estimador bayesiano para los parámetros es:

$$
\hat{\theta}^{(k)}_j = \frac{N^{(k)}_j + \alpha}{\sum_m N^{(k)}_m + V \cdot \alpha}
$$

Y la inferencia se realiza mediante:

$$
\log p(y \mid \mathbf{x}) \propto \log p(y) + \sum_j x_j \cdot \log \theta^{(y)}_j
$$

---

## ⚙️ Clase `MNB`

Se implementó una clase `MNB` que encapsula el entrenamiento y predicción con el clasificador Multinomial Naive Bayes, utilizando suavizado de Laplace (Dirichlet uniforme).

### **Inicialización**

Para inicializar el modelo se utiliza:

**MNB(alpha=1.0)**

donde `alpha` es el parámetro de suavizado que regulariza las probabilidades $ \theta^{(y)}_j $.

---

### `fit(X, y)`

Entrena el modelo con:
- `X`: matriz de conteos (documentos x palabras).
- `y`: vector de etiquetas (una por documento).

Durante el entrenamiento:
- Se estima la **probabilidad a priori** de cada clase:
  
  $$
  \log p(y = k) = \log \frac{\#\text{docs con clase } k}{\text{total de docs}}
  $$

- Se acumulan los conteos totales por clase y se estima:

  $$
  \log \theta^{(k)}_j = \log \left( \frac{\text{ocurrencias de la palabra } j \text{ en clase } k + \alpha}{\text{total de palabras en clase } k + V \cdot \alpha} \right)
  $$

---

### `predict(X)`

Devuelve las clases predichas para cada documento (fila de `X`), eligiendo:

$$
\hat{y} = \arg\max_y \left[ \log p(y) + \sum_j x_j \log \theta^{(y)}_j \right]
$$

---

### `predict_proba(X)`

Devuelve la **distribución de probabilidad** sobre las clases para cada documento:

$$
p(y \mid \mathbf{x}) = \frac{e^{\text{score}_y}}{\sum_{y'} e^{\text{score}_{y'}}}
$$

donde `score_y` es el logaritmo de la probabilidad no normalizada.

---

### `score(X, y)`

Devuelve el **accuracy** sobre el conjunto de evaluación:

$$
\frac{\# \text{predicciones correctas}}{\# \text{total de ejemplos}}
$$

---

### `macro_f1(X, y)`

Devuelve el **F1 macro** promedio entre clases, penalizando la clase desbalanceada. Se usa:

$$
\text{Macro-F1} = \frac{1}{K} \sum_{k=1}^K \text{F1-score}_k
$$

---

## 📊 Aplicación típica

Este modelo es comúnmente utilizado en:
- Clasificación de texto (spam vs. ham, noticias, opiniones).
- Análisis de sentimientos.
- Detección de temas.

Gracias a su simplicidad y efectividad, MNB es un gran modelo base para tareas donde los datos son representaciones de **frecuencias de tokens discretos**.
